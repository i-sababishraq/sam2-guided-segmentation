{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f69b057",
   "metadata": {},
   "source": [
    "# SAM2 Baseline Pipeline Demonstration\n",
    "\n",
    "This notebook demonstrates the SAM2 (Segment Anything Model 2) baseline pipeline for automatic image segmentation of people and vehicles using text prompts.\n",
    "\n",
    "## Project Overview\n",
    "- **Goal**: Segment people and vehicles in CamVid dataset images\n",
    "- **Baseline Method**: Text prompt-based segmentation with SAM2\n",
    "- **Evaluation Metric**: Dice coefficient\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81363fd",
   "metadata": {},
   "source": [
    "## 1. Install and Import Dependencies\n",
    "\n",
    "First, let's import all the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca96614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path(\"/anvil/projects/x-soc250046/x-sishraq/SegmentAnythingModel\")\n",
    "sys.path.append(str(project_root))\n",
    "sys.path.append('/anvil/projects/x-soc250046/x-sishraq/SegmentAnythingModel/sam_env/lib/python3.12/site-packages')\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# SAM2 imports\n",
    "try:\n",
    "    from sam2.build_sam import build_sam2\n",
    "    from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "    print(\"âœ“ SAM2 imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"âœ— SAM2 import failed: {e}\")\n",
    "\n",
    "# Project modules\n",
    "from src.baseline import SAM2BaselineSegmenter\n",
    "from src.utils import dice_coefficient, load_image, load_mask, visualize_results\n",
    "from src.evaluation import SegmentationEvaluator\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046f5113",
   "metadata": {},
   "source": [
    "## 2. Download SAM2 Model Checkpoints\n",
    "\n",
    "The SAM2 model checkpoints should already be downloaded during setup. Let's verify they exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18b4517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if checkpoint exists\n",
    "checkpoint_path = project_root / \"checkpoints\" / \"sam2_hiera_large.pt\"\n",
    "\n",
    "if checkpoint_path.exists():\n",
    "    print(f\"âœ“ SAM2 checkpoint found at: {checkpoint_path}\")\n",
    "    print(f\"File size: {checkpoint_path.stat().st_size / (1024**3):.2f} GB\")\n",
    "else:\n",
    "    print(f\"âœ— Checkpoint not found at: {checkpoint_path}\")\n",
    "    print(\"Please download it from: https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt\")\n",
    "\n",
    "# List available config files\n",
    "config_dir = Path(\"/anvil/projects/x-soc250046/x-sishraq/SegmentAnythingModel/sam_env/lib/python3.12/site-packages/sam2\")\n",
    "config_files = list(config_dir.glob(\"*.yaml\"))\n",
    "print(f\"\\nAvailable SAM2 configs:\")\n",
    "for config in config_files:\n",
    "    print(f\"  - {config.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b4cd0a",
   "metadata": {},
   "source": [
    "## 3. Initialize SAM2 Model\n",
    "\n",
    "Let's initialize our baseline SAM2 segmenter with the correct configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19275635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the baseline segmenter\n",
    "print(\"Initializing SAM2 baseline segmenter...\")\n",
    "segmenter = SAM2BaselineSegmenter(\n",
    "    model_cfg=\"sam2_hiera_l.yaml\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "print(\"âœ“ SAM2 baseline segmenter initialized successfully!\")\n",
    "print(f\"Device: {segmenter.device}\")\n",
    "print(f\"Model config: {segmenter.model_cfg}\")\n",
    "print(f\"Text prompts configured for:\")\n",
    "for class_name, prompts in segmenter.text_prompts.items():\n",
    "    print(f\"  {class_name}: {prompts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0306e69e",
   "metadata": {},
   "source": [
    "## 4. Load and Prepare Input Image\n",
    "\n",
    "Let's create a demo image or load one from the dataset to test our segmentation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50e74be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataset is available\n",
    "dataset_path = project_root / \"data\" / \"camvid\"\n",
    "demo_image_path = None\n",
    "\n",
    "if dataset_path.exists():\n",
    "    # Look for sample images in the dataset\n",
    "    val_images_dir = dataset_path / \"val\" / \"images\"\n",
    "    if val_images_dir.exists():\n",
    "        image_files = list(val_images_dir.glob(\"*.png\")) + list(val_images_dir.glob(\"*.jpg\"))\n",
    "        if image_files:\n",
    "            demo_image_path = image_files[0]\n",
    "            print(f\"Using dataset image: {demo_image_path.name}\")\n",
    "        else:\n",
    "            print(\"No images found in val/images directory\")\n",
    "    else:\n",
    "        print(\"val/images directory not found\")\n",
    "else:\n",
    "    print(\"Dataset not found - will create a synthetic demo image\")\n",
    "\n",
    "# Load or create demo image\n",
    "if demo_image_path and demo_image_path.exists():\n",
    "    # Load real image from dataset\n",
    "    demo_image = load_image(demo_image_path)\n",
    "    print(f\"Loaded image shape: {demo_image.shape}\")\n",
    "else:\n",
    "    # Create a synthetic demo image\n",
    "    print(\"Creating synthetic demo image...\")\n",
    "    demo_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n",
    "    # Add some simple shapes to simulate objects\n",
    "    cv2.rectangle(demo_image, (100, 200), (200, 400), (0, 255, 0), -1)  # Green rectangle (person)\n",
    "    cv2.rectangle(demo_image, (300, 300), (500, 450), (255, 0, 0), -1)  # Blue rectangle (vehicle)\n",
    "    print(f\"Created synthetic image shape: {demo_image.shape}\")\n",
    "\n",
    "# Display the demo image\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(demo_image)\n",
    "plt.title(\"Demo Image for Segmentation\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a18a1e",
   "metadata": {},
   "source": [
    "## 5. Create Image Predictor\n",
    "\n",
    "Now let's set up the SAM2 predictor with our demo image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27da831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the image in the predictor\n",
    "if segmenter.predictor is not None:\n",
    "    print(\"Setting image in SAM2 predictor...\")\n",
    "    segmenter.predictor.set_image(demo_image)\n",
    "    print(\"âœ“ Image set successfully in predictor\")\n",
    "    \n",
    "    # Get image embedding info\n",
    "    print(f\"Image shape processed: {demo_image.shape}\")\n",
    "    print(f\"Predictor ready for inference\")\n",
    "else:\n",
    "    print(\"âœ— SAM2 predictor not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd66dab",
   "metadata": {},
   "source": [
    "## 6. Generate Segmentation Masks\n",
    "\n",
    "Let's test segmentation for both people and vehicles using our baseline text prompt method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214e161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test segmentation for different classes\n",
    "target_classes = ['person', 'vehicle']\n",
    "\n",
    "print(\"Running baseline segmentation...\")\n",
    "segmentation_results = segmenter.segment_image(demo_image, target_classes)\n",
    "\n",
    "print(\"\\nSegmentation Results:\")\n",
    "for class_name, mask in segmentation_results.items():\n",
    "    mask_area = np.sum(mask) / (mask.shape[0] * mask.shape[1]) * 100\n",
    "    print(f\"{class_name}:\")\n",
    "    print(f\"  Mask shape: {mask.shape}\")\n",
    "    print(f\"  Mask area: {mask_area:.2f}% of image\")\n",
    "    print(f\"  Unique values: {np.unique(mask)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5b0c6a",
   "metadata": {},
   "source": [
    "## 7. Visualize Results\n",
    "\n",
    "Let's create comprehensive visualizations of our segmentation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1904d602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Original image\n",
    "axes[0, 0].imshow(demo_image)\n",
    "axes[0, 0].set_title(\"Original Image\", fontsize=14)\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Person mask\n",
    "person_mask = segmentation_results.get('person', np.zeros_like(demo_image[:,:,0]))\n",
    "axes[0, 1].imshow(person_mask, cmap='gray')\n",
    "axes[0, 1].set_title(\"Person Mask (Baseline)\", fontsize=14)\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Vehicle mask\n",
    "vehicle_mask = segmentation_results.get('vehicle', np.zeros_like(demo_image[:,:,0]))\n",
    "axes[0, 2].imshow(vehicle_mask, cmap='gray')\n",
    "axes[0, 2].set_title(\"Vehicle Mask (Baseline)\", fontsize=14)\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# Combined mask\n",
    "combined_mask = person_mask.astype(np.float32) + 2 * vehicle_mask.astype(np.float32)\n",
    "axes[1, 0].imshow(combined_mask, cmap='viridis')\n",
    "axes[1, 0].set_title(\"Combined Masks\\n(Person=1, Vehicle=2)\", fontsize=14)\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# Overlay on original\n",
    "overlay = demo_image.copy()\n",
    "overlay[person_mask > 0] = [255, 0, 0]  # Red for person\n",
    "overlay[vehicle_mask > 0] = [0, 0, 255]  # Blue for vehicle\n",
    "axes[1, 1].imshow(overlay)\n",
    "axes[1, 1].set_title(\"Overlay\\n(Person=Red, Vehicle=Blue)\", fontsize=14)\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "# Mask statistics\n",
    "stats_text = f\"\"\"Segmentation Statistics:\n",
    "\n",
    "Person Mask:\n",
    "â€¢ Area: {np.sum(person_mask)/(person_mask.shape[0]*person_mask.shape[1])*100:.2f}%\n",
    "â€¢ Non-zero pixels: {np.sum(person_mask > 0)}\n",
    "\n",
    "Vehicle Mask:\n",
    "â€¢ Area: {np.sum(vehicle_mask)/(vehicle_mask.shape[0]*vehicle_mask.shape[1])*100:.2f}%\n",
    "â€¢ Non-zero pixels: {np.sum(vehicle_mask > 0)}\n",
    "\n",
    "Total Segmented:\n",
    "â€¢ {(np.sum(person_mask > 0) + np.sum(vehicle_mask > 0))/(person_mask.shape[0]*person_mask.shape[1])*100:.2f}% of image\"\"\"\n",
    "\n",
    "axes[1, 2].text(0.05, 0.95, stats_text, transform=axes[1, 2].transAxes, \n",
    "                fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "axes[1, 2].set_xlim(0, 1)\n",
    "axes[1, 2].set_ylim(0, 1)\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"SAM2 Baseline Segmentation Results\", fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2625fbcb",
   "metadata": {},
   "source": [
    "## 8. Batch Processing Multiple Images\n",
    "\n",
    "Let's demonstrate how to process multiple images efficiently and evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6b0084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup evaluation if dataset is available\n",
    "if dataset_path.exists():\n",
    "    print(\"Setting up evaluation on CamVid dataset...\")\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = SegmentationEvaluator(str(dataset_path))\n",
    "    \n",
    "    # Define segmentation function for evaluator\n",
    "    def baseline_segmentation_function(image):\n",
    "        \"\"\"Wrapper function for evaluation\"\"\"\n",
    "        return segmenter.segment_image(image, target_classes=['person', 'vehicle'])\n",
    "    \n",
    "    # Run evaluation on a small subset for demo\n",
    "    print(\"Running baseline evaluation on 5 images...\")\n",
    "    results = evaluator.evaluate_method(\n",
    "        baseline_segmentation_function,\n",
    "        method_name=\"SAM2_Baseline_TextPrompt\",\n",
    "        target_classes=['person', 'vehicle'],\n",
    "        max_images=5,  # Limit for demo\n",
    "        save_visualizations=True,\n",
    "        viz_dir=str(project_root / \"results\" / \"baseline\" / \"demo_viz\")\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BASELINE EVALUATION RESULTS (5 images)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for class_name in ['person', 'vehicle', 'overall']:\n",
    "        if class_name in results:\n",
    "            metrics = results[class_name]\n",
    "            print(f\"\\n{class_name.upper()}:\")\n",
    "            print(f\"  Mean Dice Score: {metrics['mean_dice']:.4f} Â± {metrics['std_dice']:.4f}\")\n",
    "            print(f\"  Median Dice Score: {metrics['median_dice']:.4f}\")\n",
    "            print(f\"  Range: [{metrics['min_dice']:.4f}, {metrics['max_dice']:.4f}]\")\n",
    "            print(f\"  Images processed: {metrics['num_images']}\")\n",
    "    \n",
    "    # Save results\n",
    "    results_file = project_root / \"results\" / \"baseline\" / \"demo_results.json\"\n",
    "    evaluator.save_results(results, str(results_file))\n",
    "    \n",
    "else:\n",
    "    print(\"Dataset not available - creating synthetic batch processing demo...\")\n",
    "    \n",
    "    # Create multiple synthetic images\n",
    "    demo_images = []\n",
    "    for i in range(3):\n",
    "        # Create varied synthetic images\n",
    "        img = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n",
    "        # Add different shaped objects\n",
    "        cv2.rectangle(img, (50+i*50, 100+i*30), (150+i*50, 300+i*30), (0, 255, 0), -1)\n",
    "        cv2.rectangle(img, (200+i*80, 250), (400+i*80, 400), (255, 0, 0), -1)\n",
    "        demo_images.append(img)\n",
    "    \n",
    "    print(f\"Processing {len(demo_images)} synthetic images...\")\n",
    "    \n",
    "    # Process each image\n",
    "    batch_results = []\n",
    "    for i, img in enumerate(demo_images):\n",
    "        print(f\"Processing image {i+1}/{len(demo_images)}...\")\n",
    "        result = segmenter.segment_image(img, target_classes=['person', 'vehicle'])\n",
    "        batch_results.append(result)\n",
    "    \n",
    "    # Display batch processing summary\n",
    "    print(\"\\nBatch Processing Summary:\")\n",
    "    for i, result in enumerate(batch_results):\n",
    "        person_area = np.sum(result['person']) / (result['person'].shape[0] * result['person'].shape[1]) * 100\n",
    "        vehicle_area = np.sum(result['vehicle']) / (result['vehicle'].shape[0] * result['vehicle'].shape[1]) * 100\n",
    "        print(f\"Image {i+1}: Person={person_area:.2f}%, Vehicle={vehicle_area:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e7f4e9",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "This notebook demonstrated the SAM2 baseline pipeline for automatic segmentation of people and vehicles. \n",
    "\n",
    "### Key Features Demonstrated:\n",
    "1. âœ… SAM2 model initialization and configuration\n",
    "2. âœ… Text prompt-based segmentation (baseline method)\n",
    "3. âœ… Comprehensive visualization of results\n",
    "4. âœ… Batch processing capabilities\n",
    "5. âœ… Evaluation framework with Dice coefficient\n",
    "\n",
    "### Baseline Method Characteristics:\n",
    "- **Approach**: Uses predefined text prompts to generate point coordinates\n",
    "- **Limitations**: Simple grid-based point generation, not using actual vision-language models\n",
    "- **Evaluation**: Measured using Dice coefficient against ground truth\n",
    "\n",
    "### Next Steps:\n",
    "1. **Download CamVid Dataset**: Run `python src/data_preparation.py`\n",
    "2. **Full Baseline Evaluation**: Run `python src/baseline.py`\n",
    "3. **Develop Novel Method**: Create an improved automatic prompting strategy\n",
    "4. **Compare Methods**: Evaluate novel method against baseline\n",
    "\n",
    "### Novel Method Ideas:\n",
    "- Object detection + SAM2 (YOLO â†’ bounding boxes â†’ SAM2)\n",
    "- Semantic segmentation + SAM2 refinement\n",
    "- Attention-based smart point generation\n",
    "- Multi-scale prompting strategies\n",
    "\n",
    "**Goal**: Beat the baseline Dice score with a fully automatic method! ðŸŽ¯"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
